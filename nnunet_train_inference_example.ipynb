{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9974ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#requires you to pip install nnunetv2 and some others\n",
    "#for install details see: https://github.com/MIC-DKFZ/nnUNet\n",
    "import nnunetv2\n",
    "from nnunetv2.dataset_conversion.generate_dataset_json import generate_dataset_json\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "#sys.path.append('path_to_dir_w_scripts') #not required if in the same dir\n",
    "sys.path.append('/home/hvv/Documents/git_repo') #not required if in the same dir\n",
    "from nnunet_utils.utils import np2sitk, set_env_nnunet, write_envlines_nnunet, assign_to_gpus\n",
    "from nnunet_utils.preprocess import write_as_nnunet, nnunet_directory_structure, preprocess_data\n",
    "from nnunet_utils.run import train_single_model, nnunet_train_shell\n",
    "\n",
    "#root in what folder your nnunet data is stored\n",
    "root = '/home/hvv/Documents/nnunet'\n",
    "datano = '512' #this is an arbitrary number you can choose --> should not be the same as other studies\n",
    "project_name = 'nameyourproject'\n",
    "task = 'Task{}_nameyourproject'.format(datano) #this is also something you choose\n",
    "datasetID = 'Dataset{}_nameyourproject'.format(datano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8573f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over your dataset\n",
    "#create a training images and label folder\n",
    "\n",
    "#write your own script here loading an IMG and GT every iteration\n",
    "#something like:\n",
    "\n",
    "p_dir = 'where_you_store_your_nnunet_data_and_labels'\n",
    "\n",
    "p_data = 'path_to_source_img_lbl' #subfolders with IDs and scan and gt files\n",
    "for ID in os.listdir(p_data):\n",
    "    pid = os.path.join(p_data,ID)\n",
    "    \n",
    "    p_img = os.path.join(pid,'scan.nii.gz')\n",
    "    p_gt = os.path.join(pid,'gt.nii.gz')\n",
    "    \n",
    "    IMG = sitk.ReadImage(p_img)\n",
    "    GT = sitk.ReadImage(p_gt)\n",
    "    #this example should be applied to all your training images-labels\n",
    "    #you can also use this to preprocess your test set\n",
    "    #this is however not required\n",
    "    write_as_nnunet(IMG, GT, p_dir, ID)\n",
    "#IMG: sitk.Image with the CT/MR scan \n",
    "#GT: sitk Image with corresponding ground truth segmentations\n",
    "#p_dir: where the imagesTr and labelsTr should be stored\n",
    "#ID: ID number (including dataset name) for identification of IMG-GT pairs\n",
    "\n",
    "#sanity check to see if all images have labels\n",
    "root_images = os.path.join(p_dir,'imagesTr')\n",
    "root_gt = os.path.join(p_dir, 'labelsTr')\n",
    "img_lbl_paircount(root_images, root_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e72a1010",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this creates the nnunet directory structure inside the root folder\n",
    "nnunet_directory_structure(root,version=2)\n",
    "#make sure your data imagesTr and labelsTr folders are pasted in nnUnet_raw folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2fadeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now the scans have to be preprocessed for training\n",
    "#this is something specifically required by nnUnet\n",
    "#this may take a while, if it fails run again\n",
    "preprocess_data(root, \n",
    "                datano=datano,\n",
    "                datasetID=datasetID, #or task name in old version\n",
    "                dataset_name=project_name,\n",
    "                modalities=['BL_MR_FLAIR'] #should be a list representing each input channel --> important: should include MR or CT\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e13365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are two options to instantiate training models\n",
    "#1) one-by-one: \n",
    "#train models consecutively for each fold --> run this manually 5 times\n",
    "train_single_model(gpu=0, #each pc with a single gpu has number 0, selecting another gpu on a server is possible\n",
    "                   datasetID=datasetID, #defined above\n",
    "                   resolution='3d_fullres', #can select nnUnet config: ['2d','3d_fullres','3d_lowres', '3d_cascade_fullres'] \n",
    "                   fold=0, #start with the first fold (number 0)\n",
    "                  )\n",
    "\n",
    "\n",
    "#2) parallel across gpus: \n",
    "#2a) Create mapping: which GPU does what\n",
    "#Assign jobs to gpus: this is an equal distribution script\n",
    "#it can be wise to first check gpu availability \n",
    "#and then make your own dictionary with distribution dictionary\n",
    "#returns a dictionary with per entry:\n",
    "# gpu_number:[job1, job2] \n",
    "#where each job:\n",
    "#(resolution, fold_number)\n",
    "gpu_dct = assign_to_gpus(num_gpus, #total number of GPUs available OR a list of available GPU numbers\n",
    "                           num_folds, #number of folds to train (default=5)\n",
    "                           resolutions #list of resolutions, any from ['2d','3d_fullres','3d_lowres', '3d_cascade_fullres'] \n",
    "                                )\n",
    "\n",
    "\n",
    "#2b) Create shell script\n",
    "#create a train_job.sh shell script to run multiple folds at the same time\n",
    "#the shell script manages parallel computation across gpus\n",
    "nnunet_train_shell(datasetID=datasetID, #defined above\n",
    "                    root=root,#defined above\n",
    "                    conda_env='/path/to/miniconda3/envs/nnunetv2', #path to your environment\n",
    "                    gpu_res_fold_dct=gpu_dct, #is dictionary mapping resolutions, folds and gpus (see above)\n",
    "                    version=2)\n",
    "\n",
    "#2c) Run shell script\n",
    "#Last thing: run the shell script on the server\n",
    "#ssh to server, cd to nnunet folder then: bash train_job.sh\n",
    "#to make sure the server stays running when you close your pc\n",
    "#use tmux: https://tmuxcheatsheet.com/ and https://hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6305db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference: to predict segmentations using a trained model\n",
    "#After your model is trained these scripts can be used on new cases\n",
    "#there are two ways\n",
    "#1) Run in the python script line by line\n",
    "from nnunet_utils.infv2 import init_predictor, nnunetv2_get_props, nnunetv2_predict\n",
    "\n",
    "p_model = '/media/hvv/ec2480e5-6c18-468c-b971-5271432b386d/hvv/graph_age_data/mra_nnunet/train_data/MRA_vesselseg/nnUNet_trained_models/Dataset506_MRAvseg/nnUNetTrainer__nnUNetPlans__3d_fullres'\n",
    "#predictor = init_predictor(p_model)\n",
    "\n",
    "\n",
    "#2) Create a file with all input images similar to the imagesTr (but now imagesTs)\n",
    "#and run it in a batch at once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86ed50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db37aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_data = 'your_test_set_folder'\n",
    "\n",
    "for ID in tqdm(os.listdir(p_data)):\n",
    "    pid = os.path.join(p_crisp,ID)\n",
    "    #input file\n",
    "    file = os.path.join(pid,'scan.nii.gz')\n",
    "    \n",
    "    #output nifti segmentation and also probability output as npy\n",
    "    p_vseg_out = os.path.join(pid,'vesselseg.nii.gz')\n",
    "    p_npy_vseg = os.path.join(pid,'vesselseg')\n",
    "    \n",
    "    #sanity check to not run the same stuff twice\n",
    "    if os.path.exists(p_vseg_out) and os.path.exists(p_npy_vseg+'.npy'):\n",
    "        continue\n",
    "    #running this for loop can take long\n",
    "    #so a try-except to prevent stopping somewhere in the middle\n",
    "    try:\n",
    "        mra = sitk.ReadImage(file)\n",
    "        props = nnunetv2_get_props(mra)\n",
    "        mra_inp = np.expand_dims(sitk.GetArrayFromImage(mra),0)\n",
    "        seg = nnunetv2_predict(mra_inp,props,predictor, return_probabilities=True)\n",
    "\n",
    "        sitk.WriteImage(np2sitk(seg[0],mra),p_vseg_out)\n",
    "\n",
    "        np.save(p_npy_vseg,seg[1])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe4fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
